{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, grid_transform=None, image_transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_transform = image_transform\n",
    "        self.grid_transform = grid_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read the image\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = ImageOps.exif_transpose(Image.open(image_path).convert('RGB'))\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        # Create grid\n",
    "        _, height, width= image.shape\n",
    "        yx_grid = self._create_yx_grid(grid_size=(height, width))\n",
    "\n",
    "        assert len(yx_grid.shape) == len(image.shape)\n",
    "\n",
    "        if self.grid_transform is not None:\n",
    "            return self.grid_transform(yx_grid), image\n",
    "        else:\n",
    "            return yx_grid, image\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_yx_grid(grid_size):\n",
    "        \"\"\"\n",
    "        Creates mesh grid of normalised pixel coordinates based on matrix indexing convention\n",
    "        \"\"\"\n",
    "        h, w = grid_size\n",
    "        coords_i = np.linspace(0, 1, h, endpoint=False)\n",
    "        coords_j = np.linspace(0, 1, w, endpoint=False)\n",
    "        grid = torch.from_numpy(np.stack(np.meshgrid(coords_i, coords_j, indexing='ij'), axis=-1).astype('float32'))\n",
    "        return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hr_path, grid_transform, n_splits=0):\n",
    "        super().__init__()\n",
    "        self.hr_path = hr_path\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 1\n",
    "        self.grid_transform = grid_transform\n",
    "        self.fold = 0\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Image transformations\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        self.train_dataset = ImageDataset(self.hr_path, self.grid_transform, self.image_transforms)\n",
    "\n",
    "    def set_fold(self, fold):\n",
    "        self.fold = fold\n",
    "        self.setup()  # Re-setup the datasets for the new fold\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Encoded Features\n",
    "class FourierEncoding(object):\n",
    "    def __init__(self, num_input_channels: int, mapping_size: int = 256, scale: int = 10):\n",
    "        super(FourierEncoding, self).__init__()\n",
    "        self._num_input_channels = num_input_channels\n",
    "        self._mapping_size = mapping_size\n",
    "        self._B = np.random.randn(num_input_channels, mapping_size) * scale\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 3, 'Expected 3D input (got {}D input)'.format(len(x.shape))\n",
    "        height, width, channels = x.shape\n",
    "        assert channels == self._num_input_channels, \\\n",
    "            \"Expected input to have {} channels (got {} channels)\".format(self._num_input_channels, channels)\n",
    "        # Make shape compatible for matmul with _B.\n",
    "        # From [H, W, C] to [(H*W), C].\n",
    "        x = x.reshape(height * width, channels)\n",
    "        x = x @ self._B\n",
    "        # From [(H*W), C] to [H, W, C]\n",
    "        x = x.reshape(height, width, self._mapping_size)\n",
    "        x = 2 * np.pi * x\n",
    "        return np.concatenate((np.sin(x), np.cos(x)), axis=-1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelet Encoded Features\n",
    "class WaveletEncoding(object):\n",
    "    def __init__(self):\n",
    "        super(WaveletEncoding, self).__init__()\n",
    "        \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim: int = 3, num_layers: int = 4, num_channels: int = 256):      \n",
    "        super(NerfModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_channels = input_shape[0]\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        # Create the layers\n",
    "        for i in range(num_layers - 1):\n",
    "            self.conv_layers.append(nn.Conv2d(self.input_channels if i == 0 else num_channels, num_channels, kernel_size=1, padding=0))\n",
    "            self.conv_layers.append(nn.BatchNorm2d(num_channels))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Conv2d(num_channels, output_dim, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = F.relu(self.conv_layers[2*i](x))\n",
    "            x = self.conv_layers[2*i+1](x)\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerfTrainer(pl.LightningModule):\n",
    "    def __init__(self, input_shape, output_dim: int = 3, num_layers: int = 4, num_channels: int = 256, learning_rate: float = 1e-3):\n",
    "        super(NerfTrainer, self).__init__()\n",
    "        self.model = NerfModel(input_shape, output_dim, num_layers, num_channels)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the original image\n",
    "# original_image = ImageOps.exif_transpose(Image.open('../data/hr_ground_truth.JPG'))\n",
    "\n",
    "# # Calculate the new resolution as 70% of the original size\n",
    "# new_resolution = (int(original_image.width * 0.7), int(original_image.height * 0.7))\n",
    "\n",
    "# # Resize the image to the new resolution\n",
    "# low_res_image = original_image.resize(new_resolution)\n",
    "# low_res_image.rotate(-90)\n",
    "\n",
    "# # Save the low-resolution image\n",
    "# low_res_image.save('path_to_save_low_res_image.jpg')\n",
    "\n",
    "# print(f\"Low-resolution image saved as 'path_to_save_low_res_image.jpg'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Transform\n",
    "grid_transform = FourierEncoding(num_input_channels=2, mapping_size=128, scale=10)\n",
    "\n",
    "# Inputs\n",
    "image_inputs = [\n",
    "    '../data/low_res_train.jpg'\n",
    "]\n",
    "\n",
    "# Data Module Setup\n",
    "data_module = ImageDataModule(image_inputs, grid_transform)\n",
    "data_module.set_fold(0)\n",
    "\n",
    "# Trainer Setup\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 28880) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32me:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32me:\\Anaconda\\lib\\multiprocessing\\queues.py:108\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x , y \u001b[38;5;129;01min\u001b[39;00m data_module\u001b[38;5;241m.\u001b[39mtrain_dataloader():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Example: torch.Size([32, 3, 224, 224]) for a batch of 32 RGB images of size 224x224\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32me:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32me:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32me:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32me:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 28880) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for x , y in data_module.train_dataloader():\n",
    "    print(x.shape)  # Example: torch.Size([32, 3, 224, 224]) for a batch of 32 RGB images of size 224x224\n",
    "    print(y.shape)  # Example: torch.Size([32]) for a batch of 32 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "model = NerfModel(input_shape, output_dim=3, num_layers=4, num_channels=256)\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath=f'{args.save_dir}/fold{fold}', filename=f'model-fold{fold}-{{epoch:02d}}-{{val_loss:.2f}}', save_top_k=1, mode='min')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[early_stopping, checkpoint_callback, lr_monitor],\n",
    "    accelerator='gpu',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "# Training and Testing\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
