{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim: int = 3, num_layers: int = 4, num_channels: int = 256):      \n",
    "        super(NerfModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_channels = input_shape[0]\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        # Create the layers\n",
    "        for i in range(num_layers - 1):\n",
    "            self.conv_layers.append(nn.Conv2d(self.input_channels if i == 0 else num_channels, num_channels, kernel_size=3, padding=0))\n",
    "            self.conv_layers.append(nn.BatchNorm2d(num_channels))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Conv2d(num_channels, output_dim, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = F.relu(self.conv_layers[2*i](x))\n",
    "            x = self.conv_layers[2*i+1](x)\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerfTrainer(pl.LightningModule):\n",
    "    def __init__(self, input_shape, output_dim: int = 3, num_layers: int = 4, num_channels: int = 256, learning_rate: float = 1e-3):\n",
    "        super(NerfTrainer, self).__init__()\n",
    "        self.model = NerfModel(input_shape, output_dim, num_layers, num_channels)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.accuracy = Accuracy()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier Encoded Features\n",
    "class FourierEncoding(object):\n",
    "    def __init__(self, num_input_channels: int, mapping_size: int = 256, scale: int = 10):\n",
    "        super(FourierEncoding, self).__init__()\n",
    "        self._num_input_channels = num_input_channels\n",
    "        self._mapping_size = mapping_size\n",
    "        self._B = np.random.randn(num_input_channels, mapping_size) * scale\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 4, 'Expected 4D input (got {}D input)'.format(len(x.shape))\n",
    "        batches, height, width, channels = x.shape\n",
    "        assert channels == self._num_input_channels, \\\n",
    "            \"Expected input to have {} channels (got {} channels)\".format(self._num_input_channels, channels)\n",
    "        # Make shape compatible for matmul with _B.\n",
    "        # From [B, H, W, C] to [(B*H*W), C].\n",
    "        x = x.reshape(batches * height * width, channels)\n",
    "        x = x @ self._B\n",
    "        # From [(B*H*W), C] to [B, H, W, C]\n",
    "        x = x.reshape(batches, height, width, self._mapping_size)\n",
    "        x = 2 * np.pi * x\n",
    "        return np.concatenate((np.sin(x), np.cos(x)), axis=-1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wavelet Encoded Features\n",
    "class WaveletEncoding(object):\n",
    "    def __init__(self):\n",
    "        super(WaveletEncoding, self).__init__()\n",
    "        \n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
